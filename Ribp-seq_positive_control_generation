############################################################################
# Description: with these scripts we generated 10 random sets of 717 control 
# transcripts spanning exon-exon junctions. 
############################################################################

#SCRIPT 0

#file needed: 
GENCODEV44_knownGene_introns.bed #for making the datasets
GENCODEV44_knownGene.bed #for coordinates of exons

#to make 43bp coordinate for start and end
python_posctrl_to_make_introns_1.py
>>outputs: end_only.bed + start_only.bed

#to filter to have identical ID: 
awk 'NR==FNR{a[$4]; next} $4 in a' /Users/avanini/riboseq/Fusion_Transcripts_clean_last/positive_control/start_only.bed /Users/avanini/riboseq/Fusion_Transcripts_clean_last/positive_control/start_only.bed > 43bp_filtered_start_only.bed

awk 'NR==FNR{a[$4]; next} $4 in a' /Users/avanini/riboseq/Fusion_Transcripts_clean_last/positive_control/start_only.bed /Users/avanini/riboseq/Fusion_Transcripts_clean_last/positive_control/end_only.bed > 43bp_filtered_end_only.bed

#to have files sorted :
sort -V -k1,1 -k2,2n 43bp_filtered_end_only.bed > 43bp_filtered_sorted_end_only.bed

sort -V -k1,1 -k2,2n 43bp_filtered_start_only.bed > 43bp_filtered_sorted_start_only.bed

#to keep just the exon junctions that stay inside the an existing exon . 
bedtools intersect -a 43bp_filtered_sorted_start_only.bed -b GENCODEV44_knownGene.bed -wao  > 43bp_overlap_start_only.bed

bedtools intersect -a 43bp_filtered_sorted_end_only.bed -b GENCODEV44_knownGene.bed -wao  > 43bp_overlap_end_only.bed

join -1 4 -2 4 -o 1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,1.10,1.11,1.12,1.13,2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9,2.10,2.11,2.12,2.13 \
    -t $'\t' 43bp_overlap_start_only.bed 43bp_overlap_end_only.bed > merged_output.bed


awk -F'\t' '$13 == 43 && $26 == 43' merged_output.bed > filtered_merged_output.bed

grep -v -e '_alt' -e '_fix' /zfs/jacobs/AngelicaVanini/posctrl_db3/preparation_660/filtered_merged_output.bed > /zfs/jacobs/AngelicaVanini/posctrl_db3/preparation_660/filtered_merged_output_no_alt.bed

awk 'NR==FNR {id[$4]; next} $4 in id' filtered_merged_output_no_alt.bed 43bp_filtered_sorted_start_only.bed  > 43bp_final_start_only.bed

awk 'NR==FNR {id[$4]; next} $4 in id' filtered_merged_output_no_alt.bed 43bp_filtered_sorted_end_only.bed  > 43bp_final_end_only.bed

#SCRIPT 0.1
import pandas as pd

# Function to read the BED file and process the coordinates
def process_bed_file(input_bed, start_output, end_output):
    with open(input_bed, 'r') as file:
        lines = file.readlines()

    start_lines = []
    end_lines = []

    for line in lines:
        fields = line.strip().split('\t')
        chr, start, end, name, score, strand = fields[:6]
        
        start = int(start)
        end = int(end)
        
        # Calculate the coordinates for the start and end files
        start_start = start - 42
        start_end = start + 1
        end_start = end -1
        end_end = end + 42

        start_lines.append(f"{chr}\t{start_start}\t{start_end}\t{name}\t{score}\t{strand}\n")
        end_lines.append(f"{chr}\t{end_start}\t{end_end}\t{name}\t{score}\t{strand}\n")

    # Write the results to the output files
    with open(start_output, 'w') as file:
        file.writelines(start_lines)

    with open(end_output, 'w') as file:
        file.writelines(end_lines)

# File paths
input_bed = "/Users/avanini/riboseq/Fusion_Transcripts_clean_last/positive_control/GENCODEV44_knownGene_introns.bed"
start_output = "/Users/avanini/riboseq/Fusion_Transcripts_clean_last/positive_control/start_only.bed"
end_output = "/Users/avanini/riboseq/Fusion_Transcripts_clean_last/positive_control/end_only.bed"

# Process the BED file
process_bed_file(input_bed, start_output, end_output)

#SCRIPT 1

#!/usr/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=20
#SBATCH --mem=256GB
#SBATCH --time=1-12
#SBATCH --export=ALL

set -e  # Exit immediately if a command exits with a non-zero status

cd /zfs/jacobs/AngelicaVanini/github_pc

# Loop to create 10 sets
for i in {1..10}
do
    # Define the set name
    SET_NAME="set_717_$i"
    
    # Step 1: Generate random sets
    shuf -n 717 43bp_final_start_only.bed --random-source=<(yes $RANDOM) > ${SET_NAME}_random_start_only.bed

    # Step 2: Create ${SET_NAME}_random_end_only.bed
    awk 'NR==FNR{a[$4]; next} $4 in a' ${SET_NAME}_random_start_only.bed 43bp_final_end_only.bed > ${SET_NAME}_random_end_only.bed

    # Check if the file was created
    if [[ ! -f ${SET_NAME}_random_end_only.bed ]]; then
      echo "Error: ${SET_NAME}_random_end_only.bed was not created."
      exit 1
    fi

    # Step 3: Create ${SET_NAME}_random_start_only.bed
    awk 'FNR==NR{a[$4]=FNR; next} {print a[$4],$0}' ${SET_NAME}_random_end_only.bed ${SET_NAME}_random_start_only.bed | sort -n | cut -d' ' -f2- > ${SET_NAME}_random_start_only_sorted.bed

    # Check if the file was created
    if [[ ! -f ${SET_NAME}_random_start_only_sorted.bed ]]; then
      echo "Error: ${SET_NAME}_random_start_only_sorted.bed was not created."
      exit 1
    fi

    # Step 4: Generate fasta sequences for start and end
    bedtools getfasta -fi /zfs/jacobs/genomes/human/hg38/hg38.fa -bed ${SET_NAME}_random_start_only_sorted.bed -fo ${SET_NAME}_start_sequences.fasta -s

    bedtools getfasta -fi /zfs/jacobs/genomes/human/hg38/hg38.fa -bed ${SET_NAME}_random_end_only.bed -fo ${SET_NAME}_end_sequences.fasta -s

    # Step 5: Create ${SET_NAME}_random_common_names.bed
    awk '{print ">" $4}' ${SET_NAME}_random_start_only_sorted.bed > ${SET_NAME}_random_common_names.bed

    awk -F'[>_]' '{for(i=1;i<=NF;i++){if($i=="intron"){print $(i-1); break}}}' ${SET_NAME}_random_common_names.bed > ${SET_NAME}_random_ENST_ID.txt
done

#SCRIPT 2:

def read_ensg_enst_mapping(mapping_file):
    """Read the mapping of ENSG IDs to ENST IDs from the given file."""
    enst_to_ensg = {}
    with open(mapping_file, 'r') as f:
        for line in f:
            ensg_id, enst_id = line.strip().split()
            enst_to_ensg[enst_id] = ensg_id
    return enst_to_ensg

def update_bed_file_with_ensg_ids(mapping_file, bed_file, output_file):
    """Update the BED file with the corresponding ENSG IDs."""
    # Step 1: Read the mapping of ENST IDs to ENSG IDs
    enst_to_ensg = read_ensg_enst_mapping(mapping_file)

    # Step 2: Read lines from the BED file
    with open(bed_file, 'r') as f:
        bed_lines = [line.strip() for line in f]

    # Step 3: Modify the lines to insert ENSG IDs
    modified_lines = []
    for bed_line in bed_lines:
        if "ENST" in bed_line:
            # Extract the ENST ID from the line
            enst_id_start = bed_line.index("ENST")
            enst_id_end = bed_line.index("_", enst_id_start)
            enst_id = bed_line[enst_id_start:enst_id_end]
            
            # Find the corresponding ENSG ID
            ensg_id = enst_to_ensg.get(enst_id)
            if ensg_id:
                # Insert the ENSG ID after the ">" character
                parts = bed_line.split(">")
                modified_line = f">{ensg_id}_{parts[1]}"
                modified_lines.append(modified_line)
            else:
                modified_lines.append(bed_line)
        else:
            modified_lines.append(bed_line)

    # Step 4: Write the modified lines to the output file
    with open(output_file, 'w') as f:
        for line in modified_lines:
            f.write(line + '\n')

# Loop through the files
for i in range(1, 11):
    # Determine the mapping file for each set
    mapping_file = f"/zfs/jacobs/AngelicaVanini/github_pc/mart_export-{i}.txt"
    
    bed_file = f"/zfs/jacobs/AngelicaVanini/github_pc/set_717_{i}_random_common_names.bed"
    output_file = f"/zfs/jacobs/AngelicaVanini/github_pc/set_717_{i}_random_common_names_bothID.bed"
    
    # Update the BED file with ENSG IDs
    update_bed_file_with_ensg_ids(mapping_file, bed_file, output_file)

#SCRIPT 3: 
#!/bin/bash

for i in {1..10}; do
    bed_file="/zfs/jacobs/AngelicaVanini/github_pc/set_717_${i}_random_common_names_bothID.bed"
    start_fasta="/zfs/jacobs/AngelicaVanini/github_pc/set_717_${i}_start_sequences.fasta"
    end_fasta="/zfs/jacobs/AngelicaVanini/github_pc/set_717_${i}_end_sequences.fasta"
    start_output="set_717_${i}_ID_start_sequences.fasta"
    end_output="set_717_${i}_ID_end_sequences.fasta"

    awk -v bed_file="$bed_file" '
    BEGIN {
        # Read the BED file into an array
        while ((getline line < bed_file) > 0) {
            headers[++header_index] = line
        }
        close(bed_file)
        header_index = 1
    }
    {
        if ($0 ~ /^>/) {
            print headers[header_index]
            header_index++
        } else {
            print $0
        }
    }
    ' "$start_fasta" > "$start_output"

    awk -v bed_file="$bed_file" '
    BEGIN {
        # Read the BED file into an array
        while ((getline line < bed_file) > 0) {
            headers[++header_index] = line
        }
        close(bed_file)
        header_index = 1
    }
    {
        if ($0 ~ /^>/) {
            print headers[header_index]
            header_index++
        } else {
            print $0
        }
    }
    ' "$end_fasta" > "$end_output"
done


#SCRIPT 4: 
def merge_fasta_sequences(file1, file2, output_file):
    with open(file1, 'r') as f1, open(file2, 'r') as f2, open(output_file, 'w') as out:
        while True:
            header1 = f1.readline().strip()
            seq1 = f1.readline().strip()
            header2 = f2.readline().strip()
            seq2 = f2.readline().strip()
            
            if not header1 or not header2:  # Check for end of file
                break
            
            if header1 != header2:
                print(f"Warning: Headers do not match: {header1} != {header2}")
            
            out.write(f"{header1}\n")
            out.write(f"{seq1}{seq2}\n")

# Loop through the files
for i in range(1, 11):
    file1 = f"/zfs/jacobs/AngelicaVanini/github_pc/set_717_{i}_ID_start_sequences.fasta"
    file2 = f"/zfs/jacobs/AngelicaVanini/github_pc/set_717_{i}_ID_end_sequences.fasta"
    output_file = f"/zfs/jacobs/AngelicaVanini/github_pc/set_717_{i}_merged_sequences.fasta"
    
    # Run the function to merge sequences
    merge_fasta_sequences(file1, file2, output_file)







